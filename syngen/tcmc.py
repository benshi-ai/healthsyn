import csv
import yaml
import itertools

import numpy as np
from math import isinf
from numpy import ndarray
from numpy.random import default_rng, Generator

from typing import Iterable
from .types import ChainDict, Chain


def csv_reader(filename: str) -> Iterable[dict[str, str]]:
    """Read the CSV as an iterable of dicts.

    Parameters
    ----------
    filename: str
        The path to a CSV file.

    Yields
    ------
    row: dict[str, ...]
        A record from the CSV with field names taken from the header of the CSV.
    """
    with open(filename, "rt", newline="") as f:
        it = csv.reader(f, delimiter=",")

        # get the header then represent rows a dicts
        header = next(it)
        for row in it:
            yield dict(zip(header, row, strict=True))


def array_from_csv(filename: str, default: float = 0.0) -> tuple[list[str], ndarray]:
    """Read CSV into a dense 2d array

    Parameters
    ----------
    filename: str
        The path to a CSV file.

    Returns
    -------
    names: list of str, length = n_names
        The pooled row and column names.

    array: ndarray, shape = (n_names, n_names)
        The 2d array loaded from the CSV
    """

    # CSV are non-sparse, but we collect the values in a dict-of-dicts form
    rows, colnames = {}, set()
    for row in csv_reader(filename):
        # the empty field contains the name of the row, ...
        name = row.pop("")
        if name in rows:
            raise ValueError(f"Duplicate row {name} in {filename}")

        # ... but the remaining fields are values
        colnames.update(row.keys())

        # if a cell is empty, then we assume that it defaults to zero
        rows[name] = {k: float(c or default) for k, c in row.items()}

    # pool the row and col names into a list and fix their order
    names = sorted(rows.keys() | colnames)

    # fill the square matrix
    array = np.full((len(names), len(names)), default, float)
    for j, name in enumerate(names):
        row = rows.get(name, {})
        array[j] = [row.get(n, default) for n in names]

    return names, array


def from_compact(compact: ndarray) -> tuple[ndarray, ndarray]:
    """Convert from compact representation to hold-jump representation.

    Parameters
    ----------
    compact: ndarray, shape = (..., n_states, n_states)
        The square matrix with non-negative values that specify the parameters
        of the Markov chain.

        The values on the main diagonal represent the average time the chain
        spends in a given state before switching to another state (holding time).
        The state holding times are independent exponential random variables.

        The off-diagonal values specify the transition probabilities for moving
        between states: column correspond to source states and rows -- to
        target states.

    Returns
    -------
    hold: ndarray, shape = (..., n_states)
        The vector of average state holding times.

    jump: ndarray, shape = (..., n_states, n_states)
        The column-stochastic transition probability matrix.
    """
    if (compact < 0).any():
        raise ValueError("Compact representation has -ve values.")

    # the jump is everything off-digonal, possibly with unnormalized columns
    jump = compact.copy()
    ix = np.arange(jump.shape[-1])
    jump[..., ix, ix] = 0.0
    jump /= jump.sum(-2, keepdims=True)

    # extract holding times from the matrix
    hold = compact.diagonal(0, -2, -1).copy()
    return hold, jump


def read_csv(filename: str) -> Chain:
    """Read the mean matrix specification of a Markov chain from a CSV.

    Parameters
    ----------
    filename: str
        The CSV to read data from.

    Returns
    -------
    states: list of str, length = n_states
        The names of states generated by the chain.

    hold: ndarray, shape = (n_states,)
        The vector of average state holding times.

    jump: ndarray, shape = (n_states, n_states)
        The column-stochastic transition probability matrix.
    """

    states, arr = array_from_csv(filename)
    hold, jump = from_compact(arr)
    return Chain(hold=hold, jump=jump, init=0, labels=states)


def from_dict(
    hold: dict[str, float],
    jump: dict[str, dict[str, float]],
    init: str,
) -> Chain:
    """Rebuild a rate-jump representation from a mapping of partial mappings.

    Parameters
    ----------
    hold: dict
        The mapping from states to average holding times.

    jump: dict of dict
        The mapping from a source state to a dict of destination states and
        their probabilities. Determines the set of states in the chain.

    init: str
        The label of the chain's initial state.

    Returns
    -------
    hold: ndarray, shape = (n_states,)
        The vector of average state holding times.

    jump: ndarray, shape = (n_states, n_states)
        The column-stochastic transition probability matrix.

    init: int
        The initial state number.

    labels: list of str, length = n_states
        The names of states generated by the chain.

    See Also
    --------
    check: make sure the chain is valid.
    """
    # get all states in the dict-of-dicts
    states = set(jump.keys())
    states.update(*[t.keys() for t in jump.values()])

    # make sure that mean dict does not specify non-existent states
    bad = list(hold.keys() - states)
    if bad:
        return ValueError(f"Holding time dict has non-existent states {bad}.")

    # fix the order of the state names
    states = sorted(states)

    # get the hold vector: make sure it is non -ve
    nd_hold = np.array([hold.get(k, 0.0) for k in states], float)

    # rebuild the jump matrix: outer dict are mapped to cols, inner -- rows
    nd_jump = np.zeros((len(states), len(states)))
    for j, state in enumerate(states):
        col = jump.get(state, {})
        nd_jump[:, j] = [col.get(k, 0.0) for k in states]

    return check(nd_hold, nd_jump, init, states, strict=True, copy=False)


def from_many_chain_dicts(*chains: list[dict | ChainDict]) -> Chain:
    """Align the chain specification in the provided dicts.

    Parameters
    ----------
    *chains: list of dict
        The chain specification in the dict format. See `from_dict`

    Returns
    -------
    hold: ndarray, shape = (..., n_pooled_states)
        The vector of average state holding times.

    jump: ndarray, shape = (..., n_pooled_states, n_pooled_states)
        The column-stochastic transition probability matrix.

    init: ndarray, shape = (...,)
        The starting state of each chain.

    labels: list of str, n_pooled_states
        The pooled list of state names.

    See Also
    --------
    from_dict: get the chain parameters from a dict representation.
    """
    return align(*(from_dict(**dict(c)) for c in chains), uniform=False)


def align(*chains: list[Chain], strict: bool = True, uniform: bool = True) -> Chain:
    """Align the chain specification in the provided lists.

    Parameters
    ----------
    hold: list of ndarray, shape = (n,)
        The list of vectors of average state holding times, possibly of
        different lengths.

    jump: list of ndarray, shape = (n, n)
        The lsit of square column-stochastic transition probability matrices,
        possibly of different dimensions.

    init: list of int
        The starting state of each chain.

    labels: list of str
        The list of lists of state names.

    strict: bool, default = True
        Whether to require that the chains have identical sets of state labels.

    uniform: bool, default = True
        Whether to fill distributions for state that were originally undefined
        in a chain's jump matrix with uniform transition probabilities.

    Returns
    -------
    hold: ndarray, shape = (..., n_pooled_states)
        The vector of average state holding times.

    jump: ndarray, shape = (..., n_pooled_states, n_pooled_states)
        The column-stochastic transition probability matrix.

    init: ndarray, shape = (...,)
        The starting state of each chain.

    pooled: list of str, n_pooled_states
        The pooled list of state names.
    """
    # unpack the chains
    tuples = ((c.hold, c.jump, c.init, c.labels) for c in chains)
    holds, jumps, inits, states = zip(*tuples)

    n_specs = len(holds)
    assert len(jumps) == n_specs and len(states) == n_specs

    # pool all state labels and prepare the label to index mapping
    pooled = frozenset(itertools.chain(*states))
    if strict:
        # check if some chains have incomplete set states
        bad = [j for j, ss in enumerate(map(frozenset, states)) if ss != pooled]
        if any(bad):
            incomplete = [states[j] for j in bad]
            raise ValueError(f"Chains {bad} have incomplete state sets: {incomplete}.")

    pooled = sorted(pooled)
    index = {p: j for j, p in enumerate(pooled)}

    # vectorize hold-jump and init data
    n_states = len(pooled)
    hold = np.zeros((n_specs, n_states))
    jump = np.zeros((n_specs, n_states, n_states))
    init = np.zeros(n_specs, int)
    for j in range(n_specs):
        # embedding indices
        # map the spec's positional states through labels to pooled indices
        jx = np.array([index[s] for s in states[j]])  # int -> (str -> int)

        # map the spec's init positional index to pooled index
        init[j] = jx[inits[j]]

        # take the j-th hold-jump and map them into the bigger array
        hold[j, jx] = holds[j]
        jump[np.ix_([j], jx, jx)] = jumps[j]

        # force states added to a chain to become pure sources
        if uniform:
            mx = np.ones(n_states, bool)
            mx[jx] = False
            jump[j, ..., mx] = 1 / (n_states - 1)
            jx = np.flatnonzero(mx)
            jump[j, jx, jx] = 0.0

    return Chain(hold=hold, jump=jump, init=init, labels=pooled)


def to_dict(chain: Chain) -> dict:
    """Serialize the hold and jump to a dict.

    Parameters
    ----------
    hold: ndarray, shape = (n_states)
        The vectors of average state holding times.

    jump: ndarray, shape = (n_states, n_states)
        The column-stochastic transition probability matrices.

    init: int
        The starting state of the chain.

    states: list of str, default=None
        The corresponding state names.

    Returns
    -------
    spec: dict
        Dictionary representation of the markov chain.
    """
    assert chain.jump.ndim == 2 and chain.hold.ndim == 1
    assert chain.jump.shape == (len(chain.hold), len(chain.hold))

    # use numeric state names by default
    states = chain.labels
    if states is None:
        states = list(map(str, range(chain.hold.shape[-1])))

    assert len(chain.hold) == len(states)

    # represent the column-stochastic chain.jump matrix as a dict of partial
    #  mappings: source -> (target -> p)
    outer = {}
    for u, row in enumerate(chain.jump.T):  # XXX dod is, in fact, sparse row-stochastic
        outer[states[u]] = inner = {}
        for v, p in enumerate(row):
            if p > 0:
                inner[states[v]] = float(p)  # XXX numpy scalars -->> py floats

    return dict(
        hold={states[u]: float(m) for u, m in enumerate(chain.hold)},
        jump=outer,
        init=states[chain.init],
    )


def check(
    hold: ndarray,
    jump: ndarray,
    init: int | str,
    labels: list[str] = None,
    *,
    strict: bool = True,
    copy: bool = True,
) -> Chain:
    """Validate the provided markov chain.

    Parameters
    ----------
    hold: ndarray, shape = (n_states,)
        The vector of average state holding times.

    jump: ndarray, shape = (n_states, n_states)
        The column-stochastic transition probability matrix.

    init: int or str
        The initial state label or number.

    labels: list of str, default=None
        Optional names of states generated by the chain.

    strict: bool, default=True
        Whether to forbid unnormalized jump distributions.

    copy: bool, default=True
        Whether to make a copy of the parameters.

    Returns
    -------
    hold: ndarray, shape = (n_states,)
        The vector of average state holding times.

    jump: ndarray, shape = (n_states, n_states)
        The column-stochastic transition probability matrix.

    init: int
        The initial state number.

    labels: list of str, default=None
        Names of states generated by the chain.
    """
    n_states = hold.shape[-1]
    if jump.shape[-2:] != (n_states, n_states):
        raise ValueError(
            f"Jump must be a square matrix with {hold.shape[-1] = } colunms."
        )

    if jump.shape[:-2] != hold.shape[:-1]:
        raise ValueError(
            "Batched hold and jump must have matching leading dimensions. "
            f"Got {hold.shape[:-1] = } != {jump.shape[:-2]}"
        )

    # make sure the starting state is valid
    if labels is not None:
        if init not in labels:
            raise ValueError(f"Starting state {init} not in {labels}.")

        # map the starting state from any to int (assuming uniqueness)
        init = labels.index(init)

    if not (isinstance(init, int) and -n_states <= init < n_states):
        raise ValueError(
            f"Bad starting state must be {-n_states} <= init < {n_states}. Got {init}."
        )

    init = n_states + init if init < 0 else init

    # use numeric state names by default
    if labels is None:
        labels = list(map(str, range(n_states)))

    # check if states match the labels
    if len(labels) != n_states:
        raise ValueError(f"Length mismatch in labels: {len(labels) = } != {n_states}.")

    # make a copy is asked to
    if copy:
        hold, jump = hold.copy(), jump.copy()

    # sanitize holding times and the jumping probas (column-stochastic)
    if (hold < 0).any():
        if hold.ndim == 1:
            bad = [labels[j] for j in np.flatnonzero(hold < 0)]
            raise ValueError(f"-ve average holding times for states {bad}.")

        raise ValueError("Some average holding times are -ve.")

    if np.any(jump < 0):
        if jump.ndim == 2:
            bad = [labels[j] for j in np.flatnonzero((jump < 0).any(-2))]
            raise ValueError(f"-ve jump probabilities from states {bad}.")

        raise ValueError("-ve jump probabilities detected.")

    col_sums = jump.sum(-2)
    if np.any(col_sums <= 0):
        if jump.ndim == 2:
            bad = [labels[j] for j in np.flatnonzero(col_sums <= 0)]
            raise ValueError(f"all-zero jump distribution from states {bad}.")

        raise ValueError("Some states have all-zero jump probabilities.")

    not_close_to_one = ~np.isclose(col_sums, 1)
    if strict and np.any(not_close_to_one):
        # see if the jump matrix is row-stochastic by accident
        if np.all(np.isclose(jump.sum(-1), 1)):
            raise ValueError("The jump matrix appears to be row-stochastic.")

        if jump.ndim == 2:
            bad = [labels[j] for j in np.flatnonzero(not_close_to_one)]
            raise ValueError(f"Jump probabilities from states {bad} are unnormalized.")

        raise ValueError("Some states have unnormalized jump distribution.")

    # normlaize the jump probas into column-stochastic jump matrix
    jump /= col_sums[..., np.newaxis, :]
    return Chain(hold=hold, jump=jump, init=init, labels=labels)


def read_yaml(filename: str) -> Chain:
    """Read the Markov chain from a YAML.

    Parameters
    ----------
    filename: str
        The CSV to read data from.

    Returns
    -------
    hold: ndarray, shape = (n_states,)
        The vector of average state holding times.

    jump: ndarray, shape = (n_states, n_states)
        The column-stochastic transition probability matrix.

    init: int
        The initial state number.

    labels: list of str, length = n_states
        The names of states generated by the chain.

    Notes
    -----
    See the example YML markov chain spec in the provided YAML file

    >>> from pkg_resources import resource_filename
    >>> example = resource_filename("syngen.data", "example.yaml")
    """

    with open(filename, "rt") as f:
        specs = yaml.load(f, yaml.Loader)
        return from_dict(specs["hold"], specs["jump"], specs["init"])


def is_canonical(hold: ndarray, jump: ndarray) -> bool:
    """Check if the hold-jump pair represents a canonical, non-reentrant TCMC.

    Parameters
    ----------
    hold: ndarray, shape = (n_states,)
        The vector of average state holding times.

    jump: ndarray, shape = (n_states, n_states)
        The column-stochastic transition probability matrix.

    Returns
    -------
    flag: bool
        Indicator whether the TCMC is non-reentrant.
    """
    return np.all(jump.diagonal(0, -2, -1) == 0)


def blend(
    weight: ndarray, hold: ndarray, jump: ndarray, *, conditional: bool = False
) -> tuple[ndarray, ndarray]:
    r"""Blend several chains into as ingle one using the specified weights.

    Parameters
    ----------
    weight: ndarray, shape = (n_chains,)
        The blending weights.

    hold: ndarray, shape = (n_chains, n_states)
        The vectors of average state holding times.

    jump: ndarray, shape = (n_chains, n_states, n_states)
        The column-stochastic transition probability matrices.

    conditional: bool, default = False
        Whether to use conditional blending.

    Returns
    -------
    hold: ndarray, shape = (n_states,)
        The mixed vectors of average state holding times.

    jump: ndarray, shape = (n_states, n_states)
        The mixed column-stochastic transition probability matrices.

    Notes
    -----
    Each Markov chain (embedded, in the case of a TCMC() can be thought of as
    a collection of conditional transition distributions

    .. math ::

        J_{ts} = p(\text{target} | \text{source})

    specified in the form of a directed graphlet with a single source state and
    edges into target states, weighted with outgoing probability :math:`J_{ts}`.

    Thinking from the end result, we want our blended chain to use only correctly
    defined jump distributions. If we blend two chains, in one of which the state
    `y` undefined, while in the other it is transitory, we would like `y` in the
    final blend to maintain its properties as mush as possible. For example, if
    we blend
        c_0 = {'x': {'x': 1, 'y': 0}, 'y': None}

    and
        c_1 = {'x': {'x': p, 'y': 1-p}, 'y': {'x': 1}}

    then the mixture :math:`c_w = w c_1 + (1-w) c_0`
        c_w = {
            'x': {'x': (1-w) + w p, 'y': w(1-p)},
            'y': {'x': 1}
        }

    __SEEMS__ better than
        c_w = {
            'x': {'x': (1-w) + w p, 'y': w(1-p)},
            'y': {'x': w, 'y': 1-w},
        }

    (which we would've gotten had we naively filled in the undefined graphlet
    `y` in :math:`c_0` with `{'x': 0, 'y': 1}`, which we are free to do, since
    the state `y` is undefined, and never visited in `c_0` anyway).

    Therefore the blending should be performed on the level of shared transition
    graphlets with weights recomputed for each particular graphlet based on the
    chains that __actually share__ it (as opposed to naively blending on the
    entire-chain level). After we obtain blended graphlets, joining them into
    a chain is trivial -- we can just paste them, since they are just a bunch
    of conditional distributions, that hopefully, but not necessarily, connect
    into a chain.

    Consider chains :math:`C` to be blended with weights :math:`w \in [0,1]`,
    :math:`\sum_k w_k = 1` into a single `c_w = blend(C, w)`. Suppose :math:`C_x`
    are the chains from :math:`C` that __DEFINE__ a graphlet with the source `x`
    (:math:`C_x = {k \in C : \sum_t J^k_{t x} > 0}`), then the resulting blended
    graphlet is mixed from :math:`(J^k_{\cdot x})_{k \in C_x}` with weights

    .. math ::

        w_{x k} = \frac{w_k}{\sum_{p \in C_x} w_p} \,.

    The mixture :math:`c_w`

    .. math::

        J^w_{\cdot x} = \sum_{k \in C_x} w_{xk} J^k_{\cdot x} \,,

    thus inherits the graphlet distributions only from those chains that have
    them effectively defined.

    In the unfortunate case when the denominator in :math:`w_{x k}` is null,
    we mark the blended graphlet as undefined (which corresponds to the all-zero
    column in the jump matrix).
    """

    # make sure weight is a properly normalized mixture vector
    weight = np.reshape(weight, (-1, 1)).astype(float, copy=True)
    weight /= weight.sum(-2)

    # ensure the chains-dimension is flattened
    *head, n_states = hold.shape
    hold = hold.reshape(-1, n_states)
    jump = jump.reshape(-1, n_states, n_states)
    assert len(weight) == len(hold) == len(jump)

    # if a state is inadmissible in a chain, then the jump distribution from
    #  it is undefined, hence must not participate in blending
    if conditional:
        # chains and source state with defined jump distribution
        defined = jump.sum(axis=-2) > 0  # XXX n_chains x n_states

        # compute the normalizing constant for each state
        # XXX if `defined` is all False along some column, then we get zero
        #     mass column. refer to `np.add.reduce` and `np.add.identity`
        weights = weight.repeat(n_states, axis=-1)  # XXX n_chains x n_states
        mass = np.sum(weights, where=defined, axis=-2, keepdims=True)

        # we mix only well-defined jump distributions (transition graphlets),
        #  but if it happens that the relevant mixture weight sum to zero,
        #  then we make the graphlet undefined
        defined &= mass > 0
        weight = np.divide(weight, mass, where=defined, out=np.zeros_like(weights))
        # XXX weight[c, x] -- weight of chain `c` in the blended graphlet `x`

    hold = np.einsum("ku, ku -> u", weight, hold)
    jump = np.einsum("kv, kuv -> uv", weight, jump)
    return hold, jump


def rescale(hold: ndarray, jump: ndarray) -> tuple[ndarray, ndarray, ndarray]:
    """Rescale the non-transitory holding times.

    Parameters
    ----------
    hold: ndarray, shape = (..., n_states)
        The vectors of average state holding times.

    jump: ndarray, shape = (..., n_states, n_states)
        The column-stochastic transition probability matrices.

    Returns
    -------
    scale: ndarray, shape = (...)
        The new holding time scale.

    hold: ndarray, shape = (..., n_states)
        The rescaled vectors of average state holding times.

    jump: ndarray, shape = (..., n_states, n_states)
        The column-stochastic transition probability matrices (unchanged).
    """
    # get the least non-zero holding time
    scale = np.min(hold, axis=-1, initial=np.inf, where=hold > 0)

    # rescale the holding times
    return scale, hold / scale[..., np.newaxis], jump


def generator(hold: ndarray, jump: ndarray) -> ndarray:
    """Get infinitesimal generators for the hold vectors and jump matrices.

    Parameters
    ----------
    hold: ndarray, shape = (..., n_states)
        The vectors of average state holding times.

    jump: ndarray, shape = (..., n_states, n_states)
        The column-stochastic transition probability matrices.

    Returns
    -------
    inf: ndarray
        The infinitesimal probability flow generator matrix for the
        forward Kolmogorov eqn.
    """
    # convert the hold vector to the rate vector
    rate = np.full_like(hold, np.inf)
    rate = np.reciprocal(hold, where=hold > 0, out=rate)

    # guard against infinite-hold self-looping states
    check = np.logical_and(jump.diagonal(0, -2, -1) > 0, np.isinf(rate))
    if check.any():
        raise ValueError("The chain contains self-looping states with infinite rate")

    # The forward Kolmogorov equation is `\dot{P} = Q P`, for a column-stochastic
    #  matrix P_{yx}(t) of P(X(t) = y | X(0) = x).
    # XXX the solution of a time-homogeneous system of ODE `\partial_1 x(t) = A x(t)`
    #  has the form `x(t) = x(0) \exp{t A}`, where `exp` is the matrix exponential
    #  operator, computed via the Jordan decomposition of `A = V J V^{-1}`, where
    #  `J` is the unique Jordan normal-form block-diagonal matrix of `A`. In
    #  practice, JD is numerically unstable, so `scipy.linalg.expm` function
    #  uses a rational approximation to e^x and scaling-squaring
    #   [approach](https://epubs.siam.org/doi/10.1137/09074721X).
    pass

    # \sum_y J_{yx} = 1 for all x, J_{yx} \in [0, 1]
    # Q_{yx} = \lambda_x J_{yx} for y ≠ x
    # Q_{yx} = \lambda_x (J_{yx} - 1) for y = x
    inf = np.zeros_like(jump)
    inf = np.multiply(jump, rate[..., np.newaxis, :], where=jump > 0, out=inf)

    ix = np.arange(inf.shape[-1])
    inf[..., ix, ix] -= rate  # XXX in INF-rate self looping case we get an INF-INF

    assert not np.isnan(inf).any()

    return inf


def normalize(hold: ndarray, jump: ndarray) -> tuple[ndarray, ndarray]:
    """Remove purely transitory states from the chain.

    Parameters
    ----------
    hold: ndarray, shape = (n_states,)
        The vector of average state holding times.

    jump: ndarray, shape = (n_states, n_states)
        The column-stochastic transition probability matrices.

    Returns
    -------
    mask: ndarray, shape = (n_states,)
        The mask of the non-transitory states, that were kept in the chain.

    hold: ndarray, shape = (n_states,)
        The vector of average state holding times in the reduced chain.

    jump: ndarray, shape = (n_states, n_states)
        The column-stochastic transition probability matrix of the reduced chain.
    """
    fx = hold > 0
    if fx.all():
        return fx, hold, jump

    if not fx.any():
        raise ValueError("All states in the chain are transitory.")

    # guard against self-looping transitory (infinite-rate) states
    check = np.logical_and(jump.diagonal(0, -2, -1) > 0, ~fx)
    if check.any():
        raise ValueError("The chain contains self-looping transitory states.")

    # get the T principal block and subtract eye form it
    # XXX Let T = {k: r_k = \infty} be the set of transitory states, and J_{TT}
    #  be the block of the jump kernel, corresponding to probabilities J_{yx}
    #  for y, x \in T, i.e. staying within T.
    t_block = jump[np.ix_(~fx, ~fx)]

    # detect if inf-rate state are absorbing
    if np.allclose(t_block.sum(-2), 1.0):
        raise ValueError("The transitory states are absorbing.")

    # adjust the jump matrix with all possible paths paths that pass through the
    #  transitory states. This is achieved by the following extra term
    #    J_{.T} (I + J_{TT} + J_{TT}^2 + ... + J_{TT}^k + ...) J_{T.} ,
    #  where for `k\geq 0` the product J_{.T} J_{TT}^k J_{T.} gives us the
    #  probability of (right-to-left) entering T from elsewhere, staying inside
    #  T for the next k transitions, and then leaving. Since the matrix is
    #  stochastic and J_{xx} = 0 for x \in T, the T-block's operator norm is
    #  less than 1 (verify?), meaning that the Neumann's series converges to
    #  the resolvent (I - J_{TT})^{-1} .
    # Hence out adjusted jump matrix, which fast-forwards through transitory
    #  states is given by
    #    S_{yx} = J_{yx} + J_{yT} (I - J_{TT})^{-1} J_{Tx}
    # TODO this solves for the fixed point `g_x = J_{Tx} + J_{TT} g_x` where
    #    `g_x` is the vector of ... interpretation?
    jx = np.arange(len(t_block))
    t_block[jx, jx] -= 1.0  # XXX gives J_{TT} - I

    # compute the adjustment term and get the fast-forwarded jump matrix
    # XXX the values of `ffwd` make sense only in the complementary principal block
    ffwd = jump + jump[:, ~fx] @ np.linalg.solve(-t_block, jump[~fx, :])

    # reduce the matrix and vector and return the mask
    return fx, hold[fx], ffwd[np.ix_(fx, fx)]


def safe_stationary(hold: ndarray, jump: ndarray) -> ndarray:
    r"""Safely compute the stationary distributions of a TCMC.

    Parameters
    ----------
    hold: ndarray, shape = (n_states,)
        The vector of average state holding times.

    jump: ndarray, shape = (n_states, n_states)
        The column-stochastic transition probability matrices.

    Returns
    -------
    probas: ndarray, shape = (n_states, n_components)
        The stationary distributions of the TCMC.

    Notes
    -----
    If we clipping the infinities to a large numerical value, we risk getting
    instability in eigen-decomposition of the infinitesimal generator, and we
    change the relative tolerances for zero eigenvalues.

    Given an infinitesimal generator :math:`Q` (in column-stochastic convention),
    a stationary state distribution is probability vector :math:`\eta` such that

    .. math ::

        \eta = P(t) \eta = \exp\{t Q\} \eta \,, \forall{t \geq 0}

    This is equivalent to :math:`Q \eta = 0`, i.e. :math:`\eta`'s being an
    eigenvector of :math:`Q` associated with the zero eigenvalue.

    A Markov chain is irreducible if for any state is eventually reachable from
    any other state. Irreducible chains have a unique stationary distribution,
    but reducible may have more.
    """

    # skip inf-rate states, since they can never have +ve stationary probas
    # XXX normalized matrices are more well-behaved for eigen-decomposition
    fx, *normalized = normalize(hold, jump)

    # get the zero-eigen vectors of the infinitesimal generator of the chain
    # XXX we compute the RIGHT eigenvector, since we obey column-stochastic
    #  convention (unlike the left eigen-pair in thm 4.28 p.142 of
    #  [SBA.pdf](https://u.math.biu.ac.il/~amirgi/SBA.pdf))
    # XXX It can be shown that the cols of V from Q = U s V^T corresponding to
    #  zero singular values are the orthonormal basis of Q's right-nullspace.
    #  Since svd and eig are both O(n^3), we go with SVD because it won't
    #  produce complex valued vectors for real-valued matrices.
    _, s, vt = np.linalg.svd(generator(*normalized))
    eta = vt[np.isclose(s, 0)].T

    # (optional) clean the machine zeros, normalize to probability simplex,
    #  and prettify exact -ve zeros
    eta[np.isclose(eta, 0)] = 0.0
    eta /= eta.sum(-2, keepdims=True)
    eta[eta == 0] = 0.0

    # carefully place the reduced invariant distribution
    probas = np.zeros(hold.shape + eta.shape[-1:])
    probas[fx] = eta
    return probas


# (jx, t_0 and t_1): the chain is at `jx` over [t_0, t_1)
Runtime = tuple[int, float, float]


def step(random: Generator, runtime: Runtime, x: tuple[ndarray, ndarray]) -> Runtime:
    """Singe step through a Time Continuous Markov Chain with externalized state.

    Parameters
    ----------
    runtime: Runtime
        The current externalized runtime state of the tcmc.

    x: Input
        The current hold and jump parameters of the TCMC. May be constant,
        or change from one step to the next step for a non-stationary chain.

        hold: ndarray, shape = (n_states,)
            The vector of average state holding times.

        jump: ndarray, shape = (n_states, n_states)
            The column-stochastic transition probability matrices.

    random: Generator
        The prng Generator to use for drawing the holding time and the next state.

    Returns
    -------
    runtime: Runtime
        The updated externalized runtime state of the tcmc.

    Notes
    -----
    We fully embrace the markovian structure and externalize the state. This
    procedure keeps the state in `runtime`:

        `step(random, runtime, (H, J)) -> runtime'

    In general, `runtime` should be treated as an opaque object, but this
    stepper is okay with consumers tinkering with the externalized state.
    `random` is updated inplace!
    """

    # TCMC path is constructed from the embedded Markov chain (jump) and
    #  from the exponential holding times (hold)
    hold, jump = x

    assert jump.ndim == 2 and hold.ndim == 1
    assert jump.shape == (len(hold), len(hold))

    # prior `until` is the new `start`
    ux, alpha, start = runtime
    if isinf(alpha) and alpha < 0:
        # we switch INTO `ux`, if the previous interval started at -ve infinity
        vx = ux

    else:
        # jump to the next state from the current `state`
        # XXX we explicitly cast numpy scalars to python built-ins
        vx = int(random.choice(jump.shape[-2], p=jump[:, ux]))

    # the chain __holds__ state `vx` during the `[start, until)` interval
    until = start + float(random.exponential(hold[vx]))
    runtime = vx, float(start), until

    # return the externalized state carried over to the next step
    return runtime


def single_path(
    hold: ndarray,
    jump: ndarray,
    init: int,
    until: float = None,
    *,
    minimal_holding_time: float = 0.0,
    seed: int = None,
) -> Iterable[Runtime]:
    """A single path generator for Time Continuous Markov Chain.

    Parameters
    ----------
    hold: ndarray, shape = (n_states,)
        The vector of average state holding times.

    jump: ndarray, shape = (n_states, n_states)
        The column-stochastic transition probability matrices.

    init: int
        The initial state of the chain's path.

    until: float
        The time limit, until which the chain is run.

    minimal_holding_time: float
        The the smallest time between state transitions.

    seed: int, default=None
        The seed to use for PRNG.

    Yields
    ------
    state: int
        The current state of the chain.

    start: float
        The subjective time, when the chain entered the current state.

    until: float
        The subjective time, until which the chain will remain in the current state.
    """
    random = default_rng(seed)
    until = float("inf") if until is None else until

    # construct the initial state
    state, t_0, t_1 = init, -np.inf, 0.0

    # generate tcmc path
    while t_0 < until:
        # ensure minimal holding time and yield
        t_1 = max(t_1, minimal_holding_time + t_0)

        # use non None runtime, provided by the consumer through `.send`
        runtime = yield state, t_0, t_1
        if runtime is not None:
            state, t_0, t_1 = runtime

        # step through the chain
        (state, t_0, t_1), _ = step(random, (state, t_0, t_1), (hold, jump))
